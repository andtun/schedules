{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.polynomial.chebyshev as cheb\n",
    "from scipy.stats import moment\n",
    "from itertools import permutations\n",
    "#from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout = 120\n",
    "LAMBDA = 0.01\n",
    "MU = 100\n",
    "SIGMA = 40\n",
    "VARK = 1\n",
    "\n",
    "def r(task):\n",
    "    ind = 0\n",
    "    if isinstance(task, TaskSet):\n",
    "        return task.array[ind]\n",
    "    return task[ind]\n",
    "\n",
    "def p(task):\n",
    "    ind = 1\n",
    "    if isinstance(task, TaskSet):\n",
    "        return task.array[ind]\n",
    "    return task[ind]\n",
    "\n",
    "def d(task):\n",
    "    ind = 2\n",
    "    if isinstance(task, TaskSet):\n",
    "        return task.array[ind]\n",
    "    return task[ind]\n",
    "\n",
    "def remove(arr, elem):\n",
    "    return np.delete(arr, np.where(np.in1d(arr, elem)))\n",
    "\n",
    "class TaskSet:\n",
    "    \n",
    "    def __init__(self, a):\n",
    "        if isinstance(a, int):\n",
    "            rs = np.cumsum(np.random.exponential(scale=1/LAMBDA, size=(a,)))\n",
    "            ps = np.clip(np.random.normal(MU, SIGMA, size=(a,)), a_min=0, a_max=None)\n",
    "            ds = [r + VARK*moment(ps, moment=2) for r in rs]\n",
    "            self.array = np.array([rs, ps, ds]).T.astype(float)\n",
    "        else:\n",
    "            self.array = np.copy(a)\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return \"  r  |  p  |  d  \\n\" + str(self.array)\n",
    "    \n",
    "    def copy(self):\n",
    "        return TaskSet(self.array)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return TaskSet(self.array[key])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.array)\n",
    "    \n",
    "    def C(self, i, tau=0):\n",
    "        t = tau\n",
    "        for task in self.array[:i+1]:\n",
    "            if t < r(task): t = r(task)\n",
    "            t += p(task)\n",
    "        return t\n",
    "    \n",
    "    def C_max(self, tau=0):\n",
    "        t = tau\n",
    "        for task in self.array:\n",
    "            if t < r(task): t = r(task)\n",
    "            t += p(task)\n",
    "        return t\n",
    "    \n",
    "    def L(self, i=None, tau=0):\n",
    "        if i is None:\n",
    "            return self.C_max(tau) - d(self[-1])\n",
    "        return self.C(i, tau) - d(self[i])\n",
    "    \n",
    "    def L_max(self, tau=0):\n",
    "        if len(self) == 0: return float('inf')\n",
    "        return max([self.L(i, tau) for i, _ in enumerate(self)])\n",
    "    \n",
    "    def T(self, i=None, tau=0):\n",
    "        return max(0, self.L(i, tau))\n",
    "    \n",
    "    def T_max(self, tau=0):\n",
    "        return max(0, self.L_max(tau))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.array == other\n",
    "    \n",
    "    def without(self, indexes):\n",
    "        return TaskSet(np.delete(self.array, np.array(indexes).astype(float), axis=0))\n",
    "    \n",
    "    def find(self, item):\n",
    "        return np.where((self.array == item).all(axis=1))[0]\n",
    "    \n",
    "    def transpose(self):\n",
    "        return self.array.T\n",
    "    \n",
    "    def scale_r(self, alpha):\n",
    "        self.array[:,0] = self.array[:,0]*alpha\n",
    "        return self\n",
    "    \n",
    "def dual(N, tau, B):\n",
    "    if len(N.without(B)) == 0: return float('inf')\n",
    "    pi_r = r(np.argsort(N, axis=0).transpose())\n",
    "    bestL = N[pi_r].L(tau=tau)\n",
    "    for i_k in pi_r:\n",
    "        toDrop = B.copy()\n",
    "        toDrop.append(i_k)\n",
    "        #print(toDrop)\n",
    "        s = N.without(toDrop)\n",
    "        #print(s)\n",
    "        if len(s) != 0:\n",
    "            task_l = min(s, key=r)\n",
    "            i_l = N.find(task_l)[0]\n",
    "            pi_k = remove(pi_r, [i_l, i_k])\n",
    "            pi_k = np.insert(pi_k, 0, i_l)\n",
    "            pi_k = np.append(pi_k, i_k)\n",
    "            L_k = N[pi_k].L(tau=tau)\n",
    "            if L_k < bestL:\n",
    "                bestL = L_k\n",
    "    additionalL = N[pi_r].L(i=0, tau=tau)\n",
    "    if additionalL > bestL:\n",
    "        bestL = additionalL\n",
    "    return bestL\n",
    "\n",
    "class Instance:\n",
    "    \n",
    "    def __init__(self, N, tau=0, pi=[], B=[]):\n",
    "        self.N = N.copy()\n",
    "        self.tau = tau\n",
    "        self.pi = pi.copy()\n",
    "        self.B = B.copy()\n",
    "        self.nu = dual(N, tau, B)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return TaskSet(self.N.array[key])\n",
    "        \n",
    "    def best_job(self):\n",
    "        s = self.N.without(self.B)\n",
    "        sn = s[r(s.transpose()) <= self.tau]\n",
    "        if len(sn) == 0:\n",
    "            f = min(s, key=r)\n",
    "            #self.tau = r(f)\n",
    "            #self.nu = dual(self.N, self.tau, self.B)\n",
    "        else:\n",
    "            f = min(sn, key=d)\n",
    "        return self.N.find(f)[0]\n",
    "    \n",
    "    def L(self, i=None):\n",
    "        return self[self.pi].L(i, self.tau)\n",
    "    \n",
    "    def T(self, i=None):\n",
    "        return self[self.pi].T(i, self.tau)\n",
    "        \n",
    "    def L_max(self):\n",
    "        return self[self.pi].L_max(self.tau)\n",
    "    \n",
    "    def T_max(self):\n",
    "        return self[self.pi].T_max(self.tau)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Instance:\\n\" + repr(self.N) + \"\\nnu  = \" + str(self.nu) + \"\\ntau = \" + str(self.tau) + \"\\npi  = \" + str(self.pi) + \"\\nB   = \" + str(self.B)\n",
    "    \n",
    "    \n",
    "def main(N, tau=0, verbose=False, modified=False):\n",
    "    tb = time.time()\n",
    "    b_counter = 0\n",
    "    #print(\"bi\")\n",
    "    instances = [Instance(N, tau)]\n",
    "    #print(\"ai\")\n",
    "    if modified: bestPi = list(range(len(N)))\n",
    "    else: bestPi = []\n",
    "    while len(instances) > 0:\n",
    "        ti = time.time()\n",
    "        if ti - tb > timeout:\n",
    "            return TaskSet([]), -1\n",
    "        bestInstanceIndex, bestInstance = min(enumerate(instances), key=lambda x: x[1].nu) # + N[x[1].pi].L_max(tau))\n",
    "        instances.pop(bestInstanceIndex)\n",
    "        f = bestInstance.best_job()\n",
    "        f_data = bestInstance[f]\n",
    "        N1 = bestInstance.N.without(f)\n",
    "        tau1 = max(r(f_data), bestInstance.tau) + p(f_data)\n",
    "        B1 = []\n",
    "        pi1 = bestInstance.pi.copy()\n",
    "        pi1.append(N.find(f_data)[0])\n",
    "        i1 = Instance(N1, tau1, pi1, B1)\n",
    "        N2 = bestInstance.N\n",
    "        tau2 = bestInstance.tau\n",
    "        B2 = bestInstance.B.copy()\n",
    "        B2.append(N2.find(f_data)[0]) #!\n",
    "        pi2 = bestInstance.pi\n",
    "        i2 = Instance(N2, tau2, pi2, B2)\n",
    "        instances += [i1, i2]\n",
    "        b_counter += 1\n",
    "        #print(i1)\n",
    "        if len(pi1) == len(N):\n",
    "            #print(N[bestPi].L_max(tau))\n",
    "            #print(pi1)\n",
    "            if N[pi1].L_max(tau) < N[bestPi].L_max(tau):\n",
    "                bestPi = pi1.copy()\n",
    "                if verbose: print(bestPi, '\\tLmax =', N[bestPi].L_max(tau))\n",
    "        #lb = len(instances)\n",
    "        instances = [i for i in instances if max(i.nu, N[i.pi].L_max(tau)) < N[bestPi].L_max(tau)]\n",
    "        #print(lb, len(instances))\n",
    "    return bestPi, b_counter\n",
    "        \n",
    "    \n",
    "def bruteforce(N, tau=0):\n",
    "    best_L = N.L_max(tau)\n",
    "    best_N = N.copy()\n",
    "    for perm in permutations(N):\n",
    "        s = TaskSet(perm)\n",
    "        L = s.L_max(tau)\n",
    "        if L < best_L:\n",
    "            best_L = L\n",
    "            best_N = s.copy()\n",
    "    return best_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0920 02:26:28.417073 15192 deprecation.py:506] From C:\\Users\\Admin\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(25, activation='relu', input_shape=(12,)),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='relu')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [TaskSet(4) for i in range(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [bruteforce(x_train[i]) for i in range(len(x_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [x.array.flatten() for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [TaskSet(4) for i in range(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [bruteforce(x_test[i]) for i in range(len(x_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7da23d8939e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;31m# Setup work for each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'metrics'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m         \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[1;31m# Reset the state of loss metric wrappers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m--> 199\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3069\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3070\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3071\u001b[1;33m         \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m(op_input_list)\u001b[0m\n\u001b[0;32m    457\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mTensorFlow\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m   \"\"\"\n\u001b[1;32m--> 459\u001b[1;33m   \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_input_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m_get_session\u001b[1;34m(op_input_list)\u001b[0m\n\u001b[0;32m    429\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         _SESSION.session = session_module.Session(\n\u001b[1;32m--> 431\u001b[1;33m             config=get_default_session_config())\n\u001b[0m\u001b[0;32m    432\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m   1568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m     \"\"\"\n\u001b[1;32m-> 1570\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1571\u001b[0m     \u001b[1;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\neuro\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([x_train], y_train, epochs=6, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [x.flatten() for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
